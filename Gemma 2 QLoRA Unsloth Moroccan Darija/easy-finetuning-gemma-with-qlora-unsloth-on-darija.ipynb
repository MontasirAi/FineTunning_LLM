{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U kagglehub\n!pip install -q 'transformers==4.47.1'\n!pip install  accelerate datasets peft trl bitsandbytes --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n!pip install unsloth\n!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Darija** combines elements from Arabic, Berber, French, and Spanish, making it distinct from Modern Standard Arabic (MSA). As the primary language for informal communication, it is widely used in daily life, media, and social interactions. This project aims to fine-tune **Gemma 2** specifically for tasks involving **Moroccan Darija**, addressing its unique linguistic characteristics and regional variations.","metadata":{}},{"cell_type":"markdown","source":"# **Loading Model**","metadata":{}},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nmodelName = \"/kaggle/input/gemma-2/transformers/gemma-2-9b-it/2/\"\nmax_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\ndtype = (\n    None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n)\nload_in_4bit = True\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=modelName,\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T22:19:31.374970Z","iopub.execute_input":"2025-01-15T22:19:31.375310Z","iopub.status.idle":"2025-01-15T22:21:09.857185Z","shell.execute_reply.started":"2025-01-15T22:19:31.375277Z","shell.execute_reply":"2025-01-15T22:21:09.856436Z"}},"outputs":[{"name":"stdout","text":"ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\nğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.1.5: Fast Gemma2 patching. Transformers: 4.47.1.\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebd5f9494efd463486f318f05977c46a"}},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T22:21:09.858210Z","iopub.execute_input":"2025-01-15T22:21:09.858424Z","iopub.status.idle":"2025-01-15T22:21:15.554046Z","shell.execute_reply.started":"2025-01-15T22:21:09.858405Z","shell.execute_reply":"2025-01-15T22:21:15.553336Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.1.5 patched 42 layers with 42 QKV layers, 42 O layers and 42 MLP layers.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# **Loading Dataset**","metadata":{}},{"cell_type":"code","source":"instruction = {\n    'determination':\"\"\"Transform the given input text in Darija from its indefinite forms to their corresponding definite forms in Darija.\n    Maintain the structure and order of the words as in the input.\"\"\",\n    'conj_past':\"\"\"Conjugate the given verb in Darija into its past tense for all pronouns (nta,nti,howa,hia,7na,ntoma,homa).\"\"\",\n    'conj_present':\"\"\"Conjugate the given verb in Darija into its present tense for all pronouns (ana, nta, nti, howa, hiya, 7na, ntouma, homa).\"\"\",\n    'imperative':'Generate the imperative conjugations of the given verb in Darija for specified pronouns (nta, nti, ntouma).',\n    'pluralization':\"\"\"Pluralize the given nouns in Darija.\n    Maintain the structure and order of the words as in the input.\"\"\",\n    'nominalization':\"\"\"Perform nominalization on the given verbs in Darija. Convert the verbs into their corresponding noun forms.\n    Maintain the structure and order of the words as in the input.\"\"\",\n    'name_darija_to_arab':\"\"\"Convert names from Darija to Arabic.\n    Maintain the structure and order of the words as in the input.\"\"\",\n    'darija_to_arab':\"\"\"Convert names from Darija to Arabic.\n    Maintain the structure and order of the words as in the input.\"\"\",\n    'darija_arabic_to_arabic':\"\"\"Translate words from Darija Arabic to Arabic.\n    Maintain the structure and order of the words as in the input.\"\"\",\n    'darija_arabic_to_darija':\"\"\"Translate words from Darija Arabic to Darija.\n    The input consists of words in Darija Arabic, separated by /n/n for each word.\n    Provide multiple possible ways a word can be written in Darija, if applicable.\n    Maintain the structure and order of the words as in the input.\"\"\" ,\n    'alpaca':\"\"\"Perform question answering and provide the output in Arabic\"\"\",\n    'textgen':\"\"\"Complete the text by generating a continuation for the given input.\"\"\"\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T22:45:18.261937Z","iopub.execute_input":"2025-01-15T22:45:18.262258Z","iopub.status.idle":"2025-01-15T22:45:18.266619Z","shell.execute_reply.started":"2025-01-15T22:45:18.262232Z","shell.execute_reply":"2025-01-15T22:45:18.265601Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import kagglehub\n# modified darija-eng-arabic-linguistic_dataset\npath = kagglehub.dataset_download(\"aminemontasir/moroccan-arabic-darija-task-dataset\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T22:21:15.561695Z","iopub.execute_input":"2025-01-15T22:21:15.561929Z","iopub.status.idle":"2025-01-15T22:21:15.908107Z","shell.execute_reply.started":"2025-01-15T22:21:15.561907Z","shell.execute_reply":"2025-01-15T22:21:15.907410Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from datasets import load_dataset\nds = load_dataset('FreedomIntelligence/alpaca-gpt4-arabic')\nds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T22:21:15.908854Z","iopub.execute_input":"2025-01-15T22:21:15.909071Z","iopub.status.idle":"2025-01-15T22:21:17.593464Z","shell.execute_reply.started":"2025-01-15T22:21:15.909052Z","shell.execute_reply":"2025-01-15T22:21:17.592721Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'conversations'],\n        num_rows: 49969\n    })\n})"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"def preprocess(batch):\n  batch['instruction'] = instruction['alpaca']\n  batch['input'] = batch['conversations'][0]['value']\n  batch['output'] = batch['conversations'][1]['value']\n  return batch\nds1 = ds['train'].map(preprocess,remove_columns=['conversations','id'])\nds1 = ds1.shuffle(47).select(range(5000))\nds1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T22:21:17.594309Z","iopub.execute_input":"2025-01-15T22:21:17.594601Z","iopub.status.idle":"2025-01-15T22:21:21.330480Z","shell.execute_reply.started":"2025-01-15T22:21:17.594565Z","shell.execute_reply":"2025-01-15T22:21:21.329827Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/49969 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ecc2728ca0745de8c35902916266de4"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['instruction', 'input', 'output'],\n    num_rows: 5000\n})"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"ds = load_dataset(\"csv\", data_files=\"/kaggle/input/moroccan-arabic-darija-task-dataset/darija_tasks.csv\")\nds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T22:21:21.331344Z","iopub.execute_input":"2025-01-15T22:21:21.331725Z","iopub.status.idle":"2025-01-15T22:21:21.418487Z","shell.execute_reply.started":"2025-01-15T22:21:21.331684Z","shell.execute_reply":"2025-01-15T22:21:21.417696Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['inputs', 'targets', 'types'],\n        num_rows: 24267\n    })\n})"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"def preprocess(batch):\n  batch['instruction'] = instruction[batch['types']]\n  batch['input'] = batch['inputs']\n  batch['output'] = batch['targets']\n  return batch\nds2 = ds['train'].map(preprocess,remove_columns=['inputs', 'targets', 'types'])\nds2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T22:21:21.421136Z","iopub.execute_input":"2025-01-15T22:21:21.421348Z","iopub.status.idle":"2025-01-15T22:21:21.600703Z","shell.execute_reply.started":"2025-01-15T22:21:21.421329Z","shell.execute_reply":"2025-01-15T22:21:21.600043Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['instruction', 'input', 'output'],\n    num_rows: 24267\n})"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"ds = load_dataset('AbderrahmanSkiredj1/moroccan_darija_wikipedia_dataset')\nds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T22:21:21.601824Z","iopub.execute_input":"2025-01-15T22:21:21.602172Z","iopub.status.idle":"2025-01-15T22:21:22.768876Z","shell.execute_reply.started":"2025-01-15T22:21:21.602135Z","shell.execute_reply":"2025-01-15T22:21:22.768084Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 4862\n    })\n})"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"def transform_text(batch):\n    inputs = []\n    outputs = []\n    inst = []\n    for text in batch['text']:\n        words = text.split()\n        inputs.append(\" \".join(words[:10]))  \n        outputs.append(\" \".join(words[10:])) \n        inst.append(instruction['textgen'])\n    return {'instruction':inst,'input': inputs, 'output': outputs}\nds3 = ds['train'].map(transform_text, batched=True,remove_columns=['text'])\nds3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T22:21:22.769731Z","iopub.execute_input":"2025-01-15T22:21:22.770038Z","iopub.status.idle":"2025-01-15T22:21:22.797421Z","shell.execute_reply.started":"2025-01-15T22:21:22.770004Z","shell.execute_reply":"2025-01-15T22:21:22.796826Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['instruction', 'input', 'output'],\n    num_rows: 4862\n})"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"print(ds1,ds2,ds3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T22:21:22.798161Z","iopub.execute_input":"2025-01-15T22:21:22.798444Z","iopub.status.idle":"2025-01-15T22:21:22.803684Z","shell.execute_reply.started":"2025-01-15T22:21:22.798414Z","shell.execute_reply":"2025-01-15T22:21:22.802875Z"}},"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['instruction', 'input', 'output'],\n    num_rows: 5000\n}) Dataset({\n    features: ['instruction', 'input', 'output'],\n    num_rows: 24267\n}) Dataset({\n    features: ['instruction', 'input', 'output'],\n    num_rows: 4862\n})\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from datasets import concatenate_datasets\ndataset = concatenate_datasets([ds1,ds2,ds3])\ndataset = dataset.shuffle(seed=42)\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T22:21:22.804446Z","iopub.execute_input":"2025-01-15T22:21:22.804739Z","iopub.status.idle":"2025-01-15T22:21:22.843079Z","shell.execute_reply.started":"2025-01-15T22:21:22.804718Z","shell.execute_reply":"2025-01-15T22:21:22.842310Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['instruction', 'input', 'output'],\n    num_rows: 34129\n})"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"gemma_prompt = \"\"\"<start_of_turn>user Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n### Instruction:\n{}\n\n### Input:\n{}\n\n<end_of_turn>\n<start_of_turn>model \n### Response:\n{}<end_of_turn>\"\"\"\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs       = examples[\"input\"]\n    outputs      = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = gemma_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\npass\ndataset = dataset.map(formatting_prompts_func, batched = True,)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T22:21:22.843810Z","iopub.execute_input":"2025-01-15T22:21:22.844014Z","iopub.status.idle":"2025-01-15T22:21:23.862930Z","shell.execute_reply.started":"2025-01-15T22:21:22.843996Z","shell.execute_reply":"2025-01-15T22:21:23.861754Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/34129 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2074c6086664265949acd9f88438243"}},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"# **Finetunning**","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        max_steps = 30, # for good result more than 4000\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n        report_to = \"none\", # Use this for WandB etc\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T22:23:13.479447Z","iopub.execute_input":"2025-01-15T22:23:13.479878Z","iopub.status.idle":"2025-01-15T22:23:27.090964Z","shell.execute_reply.started":"2025-01-15T22:23:13.479842Z","shell.execute_reply":"2025-01-15T22:23:27.090033Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/34129 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28398e3c32f34d9f935bb716d95a9755"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T22:23:31.758491Z","iopub.execute_input":"2025-01-15T22:23:31.758873Z","iopub.status.idle":"2025-01-15T22:31:06.042732Z","shell.execute_reply.started":"2025-01-15T22:23:31.758836Z","shell.execute_reply":"2025-01-15T22:31:06.041605Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 34,129 | Num Epochs = 1\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n\\        /    Total batch size = 8 | Total steps = 30\n \"-____-\"     Number of trainable parameters = 54,018,048\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [30/30 06:42, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>4.115400</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.845000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>3.543900</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>3.333900</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.962400</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>3.064500</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>2.563300</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>2.959900</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>2.770400</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>2.222900</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>2.133600</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>1.998000</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>2.432000</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>2.113400</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>2.224000</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>2.571300</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>1.550000</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>2.049900</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>2.657400</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.068700</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>2.114200</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>1.743100</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>2.187000</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>2.001300</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>2.213200</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>2.408600</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>1.618300</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>2.264900</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>3.027000</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.864100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# Local saving\nmodel.save_pretrained(\"saved_model\")  \ntokenizer.save_pretrained(\"saved_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T22:31:15.194160Z","iopub.execute_input":"2025-01-15T22:31:15.195371Z","iopub.status.idle":"2025-01-15T22:31:16.161108Z","shell.execute_reply.started":"2025-01-15T22:31:15.195315Z","shell.execute_reply":"2025-01-15T22:31:16.160304Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"('saved_model/tokenizer_config.json',\n 'saved_model/special_tokens_map.json',\n 'saved_model/tokenizer.model',\n 'saved_model/added_tokens.json',\n 'saved_model/tokenizer.json')"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"# **Testing**","metadata":{}},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.model_download(\"aminemontasir/gemma_2-9b_darija/transformers/default\")\n\nprint(\"Path to model files:\", path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T22:32:06.157532Z","iopub.execute_input":"2025-01-15T22:32:06.157718Z","iopub.status.idle":"2025-01-15T22:32:08.962621Z","shell.execute_reply.started":"2025-01-15T22:32:06.157700Z","shell.execute_reply":"2025-01-15T22:32:08.961645Z"}},"outputs":[{"name":"stdout","text":"Path to model files: /kaggle/input/gemma_2-9b_darija/transformers/default/1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install transformers peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T22:32:14.527772Z","iopub.execute_input":"2025-01-15T22:32:14.528084Z","iopub.status.idle":"2025-01-15T22:32:19.093670Z","shell.execute_reply.started":"2025-01-15T22:32:14.528060Z","shell.execute_reply":"2025-01-15T22:32:19.092645Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.2.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/gemma-2/transformers/gemma-2-9b-it/2/\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"/kaggle/input/gemma-2/transformers/gemma-2-9b-it/2/\",\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T22:32:32.124760Z","iopub.execute_input":"2025-01-15T22:32:32.125066Z","iopub.status.idle":"2025-01-15T22:35:16.664843Z","shell.execute_reply.started":"2025-01-15T22:32:32.125042Z","shell.execute_reply":"2025-01-15T22:35:16.664190Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf451ebcd6cd4f5994a0b1323b7ccd84"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from peft import PeftModel\n\nlora_weights_path = \"/kaggle/input/gemma_2-9b_darija/transformers/default/1\"  # Replace with the path to your LoRA weights\nmodel = PeftModel.from_pretrained(model, lora_weights_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T22:39:08.655402Z","iopub.execute_input":"2025-01-15T22:39:08.656121Z","iopub.status.idle":"2025-01-15T22:39:13.799610Z","shell.execute_reply.started":"2025-01-15T22:39:08.656091Z","shell.execute_reply":"2025-01-15T22:39:13.798946Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"gemma_prompt = \"\"\"<start_of_turn>user Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n### Instruction:\n{}\n\n### Input:\n{}\n\n<end_of_turn>\n<start_of_turn>model \n### Response:\n\"\"\"\ninput_text = gemma_prompt.format(instruction['alpaca'],\"Ø´Ø±Ø­ Ù„ÙŠØ§ ÙƒÙŠÙØ§Ø´ Ù†ØªØ¬Ø§ÙˆØ² Ø³Ø±Ø¹Ø© Ø§Ù„Ø¶Ùˆ\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T22:45:24.143507Z","iopub.execute_input":"2025-01-15T22:45:24.143807Z","iopub.status.idle":"2025-01-15T22:45:24.147914Z","shell.execute_reply.started":"2025-01-15T22:45:24.143784Z","shell.execute_reply":"2025-01-15T22:45:24.146919Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"input_ids = tokenizer(input_text, return_tensors=\"pt\").to('cuda')\noutputs = model.generate(**input_ids,max_new_tokens=364, use_cache=True)\nprint(tokenizer.decode(outputs[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T22:45:28.512167Z","iopub.execute_input":"2025-01-15T22:45:28.512482Z","iopub.status.idle":"2025-01-15T22:46:11.192712Z","shell.execute_reply.started":"2025-01-15T22:45:28.512457Z","shell.execute_reply":"2025-01-15T22:46:11.191968Z"}},"outputs":[{"name":"stderr","text":"The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n","output_type":"stream"},{"name":"stdout","text":"<bos><start_of_turn>user Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n### Instruction:\nPerform question answering and provide the output in Arabic\n\n### Input:\nØ´Ø±Ø­ Ù„ÙŠØ§ ÙƒÙŠÙØ§Ø´ Ù†ØªØ¬Ø§ÙˆØ² Ø³Ø±Ø¹Ø© Ø§Ù„Ø¶Ùˆ\n\n<end_of_turn>\n<start_of_turn>model \n### Response:\nÙˆÙÙ‚Ù‹Ø§ Ù„Ù„ÙÙŠØ²ÙŠØ§Ø¡ Ø§Ù„Ø­Ø¯ÙŠØ«Ø©ØŒ Ù…Ù† Ø§Ù„Ù…Ø³ØªØ­ÙŠÙ„ ØªØ¬Ø§ÙˆØ² Ø³Ø±Ø¹Ø© Ø§Ù„Ø¶ÙˆØ¡. ÙÙŠ Ø¹Ø§Ù… 1905ØŒ Ù†Ø´Ø± Ø£Ù„Ø¨Ø±Øª Ø£ÙŠÙ†Ø´ØªØ§ÙŠÙ† Ù†Ø¸Ø±ÙŠØ© Ø§Ù„Ù†Ø³Ø¨ÙŠØ© Ø§Ù„Ø®Ø§ØµØ©ØŒ Ø§Ù„ØªÙŠ ØªÙ†Øµ Ø¹Ù„Ù‰ Ø£Ù† Ø³Ø±Ø¹Ø© Ø§Ù„Ø¶ÙˆØ¡ ÙÙŠ Ø§Ù„ÙØ±Ø§Øº Ù‡ÙŠ Ø«Ø§Ø¨Øª Ø¹Ø§Ù„Ù…ÙŠØŒ ÙˆØªØ¨Ù„Øº Ø­ÙˆØ§Ù„ÙŠ 299ØŒ792ØŒ458 Ù…ØªØ±Ù‹Ø§ ÙÙŠ Ø§Ù„Ø«Ø§Ù†ÙŠØ©.\n\nØªØªØ¶Ù…Ù† Ù†Ø¸Ø±ÙŠØ© Ø§Ù„Ù†Ø³Ø¨ÙŠØ© Ø§Ù„Ø®Ø§ØµØ© Ø£ÙŠØ¶Ù‹Ø§ Ù…ÙÙ‡ÙˆÙ…Ù‹Ø§ ÙŠØ³Ù…Ù‰ \"Ø§Ù„ÙƒØªÙ„Ø© Ø§Ù„Ù†Ø³Ø¨ÙŠØ©\"ØŒ Ø§Ù„Ø°ÙŠ ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ø£Ù† ÙƒØªÙ„Ø© Ø§Ù„Ø¬Ø³Ù… ØªØ²Ø¯Ø§Ø¯ Ù…Ø¹ Ø²ÙŠØ§Ø¯Ø© Ø³Ø±Ø¹ØªÙ‡. Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙ‚ØªØ±Ø¨ Ø§Ù„Ø¬Ø³Ù… Ù…Ù† Ø³Ø±Ø¹Ø© Ø§Ù„Ø¶ÙˆØ¡ØŒ ØªØ²Ø¯Ø§Ø¯ ÙƒØªÙ„ØªÙ‡ Ø¨Ø´ÙƒÙ„ Ù„Ø§ Ù†Ù‡Ø§Ø¦ÙŠØŒ Ù…Ù…Ø§ ÙŠØ¬Ø¹Ù„ Ù…Ù† Ø§Ù„Ù…Ø³ØªØ­ÙŠÙ„ Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø³Ø±Ø¹Ø© Ø§Ù„Ø¶ÙˆØ¡.\n\nØ¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø°Ù„ÙƒØŒ ØªÙ†Øµ Ù†Ø¸Ø±ÙŠØ© Ø§Ù„Ù†Ø³Ø¨ÙŠØ© Ø§Ù„Ø¹Ø§Ù…Ø©ØŒ Ø§Ù„ØªÙŠ Ù†Ø´Ø±Ù‡Ø§ Ø£ÙŠÙ†Ø´ØªØ§ÙŠÙ† ÙÙŠ Ø¹Ø§Ù… 1915ØŒ Ø¹Ù„Ù‰ Ø£Ù† Ø³Ø±Ø¹Ø© Ø§Ù„Ø¶ÙˆØ¡ Ù‡ÙŠ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø³Ø±Ø¹Ø© Ø£ÙŠ Ø´ÙŠØ¡ ÙÙŠ Ø§Ù„ÙƒÙˆÙ†.\n\nÙ„Ø°Ù„ÙƒØŒ Ø¹Ù„Ù‰ Ø§Ù„Ø±ØºÙ… Ù…Ù† Ø£Ù†Ù†Ø§ Ù†Ø³ØªØ·ÙŠØ¹ Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ù†Ø³Ø¨ Ø¹Ø§Ù„ÙŠØ© Ù…Ù† Ø§Ù„Ø³Ø±Ø¹Ø©ØŒ Ø¥Ù„Ø§ Ø£Ù†Ù†Ø§ Ù„Ù† Ù†ØªÙ…ÙƒÙ† Ø£Ø¨Ø¯Ù‹Ø§ Ù…Ù† ØªØ¬Ø§ÙˆØ² Ø³Ø±Ø¹Ø© Ø§Ù„Ø¶ÙˆØ¡.<end_of_turn><eos>\n","output_type":"stream"}],"execution_count":8}]}